#!/usr/bin/env python3
"""
Session Insights - Parallel Analysis Orchestrator

Reads extracted session JSON, splits into batches, and uses `claude -p` (non-interactive mode)
to analyze each batch in true OS-level parallelism via ThreadPoolExecutor.

Usage:
    python3 session-insights-analyze.py [--input FILE] [--output FILE] [--batch-size N]
"""

import json
import os
import sys
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# claude -p (pipe mode) is non-interactive and process-isolated,
# so it's safe to run inside another Claude Code session.
# Remove CLAUDECODE env var to bypass the nested session check.
_SUBPROCESS_ENV = {k: v for k, v in os.environ.items() if k != "CLAUDECODE"}

# â”€â”€â”€ Prompt Template â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BATCH_PROMPT = """ä½ æ˜¯ session-insights çš„ä¼šè¯åˆ†æ Sub-Agentã€‚è¯·åˆ†æä»¥ä¸‹ {batch_size} ä¸ªä¼šè¯æ•°æ®ï¼Œä¸ºæ¯ä¸ªä¼šè¯ç”Ÿæˆè¯¦ç»†åˆ†æçš„ Markdown ç‰‡æ®µã€‚

## è¾“å…¥æ•°æ®

{batch_json}

## è¾“å‡ºæ ¼å¼

å¯¹æ¯ä¸ªä¼šè¯ç”Ÿæˆä»¥ä¸‹ç»“æ„ï¼ˆä¸è¦åŒ…å«ã€Œä¸‰ã€å„ä¼šè¯è¯¦æƒ…ã€ç« èŠ‚æ ‡é¢˜ï¼Œç›´æ¥ä» ### å¼€å§‹ï¼‰ï¼š

### ä¼šè¯ Nï¼š{{æ—¥æœŸ}} {{ä¸»é¢˜æ¦‚æ‹¬}}

**æ ¸å¿ƒäº¤äº’æ—¶åºå›¾**

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant A as Claude Agent
    participant S as å­ Agent

    Note over U,S: Phase 1 â€” é˜¶æ®µæè¿°
    U->>A: æ¦‚æ‹¬ç”¨æˆ·æŒ‡ä»¤
    A-->>U: ç»“æœ
```

ï¼ˆä¸è¶…è¿‡ 15 ä¸ªäº¤äº’èŠ‚ç‚¹ï¼Œç”¨ par/loop/Note over è¯­æ³•ï¼Œåˆå¹¶è¿ç»­åŒç±»æ“ä½œï¼‰

**ç»Ÿè®¡**ï¼šç”¨æˆ·æ¶ˆæ¯ X | å·¥å…·è°ƒç”¨ Y | å­ Agent Z | Git æäº¤ W

**ç²¾é€‰ç”¨æˆ·æŒ‡ä»¤**

| # | ç”¨æˆ·æŒ‡ä»¤ï¼ˆç²¾ç®€ï¼‰ | Agent å“åº” |
|---|-----------------|-----------|
ï¼ˆç²¾é€‰ 5-10 æ¡æœ€æœ‰ä»£è¡¨æ€§çš„æŒ‡ä»¤ï¼šå…³é”®éœ€æ±‚ã€Bug æŠ¥å‘Šã€æ¶æ„å†³ç­–ã€æ–¹å‘è½¬å˜ï¼‰

**Agent å›¢é˜Ÿæ‹“æ‰‘**ï¼ˆä»…å½“è¯¥ä¼šè¯æœ‰ sub_agents æ•°æ®æ—¶å±•ç¤ºï¼‰

```mermaid
flowchart TD
    Main["ğŸ¯ Main Agent"]
    Sub1["ğŸ” å­ Agent å Ã—N"]
    Main --> Sub1
    style Main fill:#f0fdfa,stroke:#0d9488,stroke-width:2px
    style Sub1 fill:#dbeafe,stroke:#3b82f6
```

ï¼ˆå¿…é¡»ç”¨ style ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾ç½®ä¸åŒ fill/stroke é¢œè‰²ï¼‰

## è§„åˆ™
- ç”¨è‡ªå·±çš„ç†è§£æ¥åˆ†æï¼Œä¸æœºæ¢°å¤åˆ¶æ•°æ®
- æ—¶åºå›¾åˆå¹¶è¿ç»­åŒç±»æ“ä½œï¼Œæ§åˆ¶åœ¨ 15 èŠ‚ç‚¹ä»¥å†…
- flowchart å¿…é¡»ç”¨ style ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾ç½®ä¸åŒé¢œè‰²
- æŒ‰ä¼šè¯æ—¶é—´é¡ºåºæ’åˆ—
- è¿”å›çº¯ Markdown ç‰‡æ®µï¼Œä¸è¦é¢å¤–è§£é‡Šæˆ–åŒ…è£¹ä»£ç å—
- è¾“å‡ºè¯­è¨€ï¼šä¸­æ–‡
"""


# â”€â”€â”€ Batch Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_batch(batch_data, batch_index, total_batches):
    """Analyze a single batch using claude -p (non-interactive mode)."""
    batch_json = json.dumps(batch_data, ensure_ascii=False, indent=2)
    prompt = BATCH_PROMPT.format(
        batch_size=len(batch_data),
        batch_json=batch_json,
    )

    try:
        proc = subprocess.run(
            ["claude", "-p"],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=300,  # 5 min per batch
            env=_SUBPROCESS_ENV,
        )

        if proc.returncode == 0 and proc.stdout.strip():
            return {
                "batch_index": batch_index,
                "status": "success",
                "content": proc.stdout.strip(),
            }
        else:
            err = proc.stderr.strip() if proc.stderr else f"exit code {proc.returncode}"
            print(f"  âš ï¸ æ‰¹æ¬¡ {batch_index}/{total_batches} é”™è¯¯: {err[:200]}", file=sys.stderr)
            return {
                "batch_index": batch_index,
                "status": "error",
                "content": generate_fallback(batch_data),
            }

    except subprocess.TimeoutExpired:
        print(f"  âš ï¸ æ‰¹æ¬¡ {batch_index}/{total_batches} è¶…æ—¶ï¼ˆ5 åˆ†é’Ÿï¼‰", file=sys.stderr)
        return {
            "batch_index": batch_index,
            "status": "timeout",
            "content": generate_fallback(batch_data),
        }
    except FileNotFoundError:
        print("  âŒ æœªæ‰¾åˆ° claude å‘½ä»¤ï¼Œè¯·ç¡®è®¤ Claude Code CLI å·²å®‰è£…", file=sys.stderr)
        return {
            "batch_index": batch_index,
            "status": "error",
            "content": generate_fallback(batch_data),
        }
    except Exception as e:
        print(f"  âš ï¸ æ‰¹æ¬¡ {batch_index}/{total_batches} å¼‚å¸¸: {e}", file=sys.stderr)
        return {
            "batch_index": batch_index,
            "status": "error",
            "content": generate_fallback(batch_data),
        }


def generate_fallback(batch_data):
    """Generate simplified summaries when a batch fails."""
    lines = ["> âš ï¸ ä»¥ä¸‹ä¼šè¯å› åˆ†æè¶…æ—¶æˆ–å¤±è´¥ï¼Œä»…å±•ç¤ºç®€è¦æ‘˜è¦\n"]
    for sess in batch_data:
        inputs = sess.get("user_inputs", [])
        topic = inputs[0][:100] if inputs else "æœªçŸ¥ä¸»é¢˜"
        lines.append(f"### ä¼šè¯ï¼š{sess.get('time_start', '?')} â€” {topic}")
        lines.append(
            f"**ç»Ÿè®¡**ï¼šç”¨æˆ·æ¶ˆæ¯ {sess.get('user_count', 0)} | "
            f"å·¥å…·è°ƒç”¨ {sess.get('tool_calls', 0)} | "
            f"å­ Agent {sess.get('subagent_count', 0)} | "
            f"Git æäº¤ {len(sess.get('git_commits', []))}"
        )
        lines.append("")
    return "\n".join(lines)


# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Session Insights - Parallel Analysis Orchestrator")
    parser.add_argument("--input", default="/tmp/session-insights-raw.json",
                        help="Input JSON from session-insights.py (default: /tmp/session-insights-raw.json)")
    parser.add_argument("--output", default="/tmp/session-insights-chapter3.md",
                        help="Output file for chapter 3 markdown (default: /tmp/session-insights-chapter3.md)")
    parser.add_argument("--batch-size", type=int, default=5,
                        help="Sessions per batch (default: 5)")
    parser.add_argument("--max-workers", type=int, default=None,
                        help="Max parallel workers (default: number of batches)")
    args = parser.parse_args()

    # â”€â”€ Read data â”€â”€
    try:
        with open(args.input, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(json.dumps({"error": f"Input file not found: {args.input}"}))
        sys.exit(1)

    sessions = data.get("sessions", [])
    if not sessions:
        print(json.dumps({"error": "No sessions in input data"}))
        sys.exit(1)

    # â”€â”€ Sort by time â”€â”€
    sessions.sort(key=lambda s: s.get("time_start", ""))

    # â”€â”€ Split into batches â”€â”€
    batches = [sessions[i:i + args.batch_size] for i in range(0, len(sessions), args.batch_size)]
    total = len(batches)
    workers = args.max_workers or total

    print(f"ğŸ“Š å¹¶è¡Œåˆ†æ: {len(sessions)} ä¸ªä¼šè¯ â†’ {total} æ‰¹ Ã— {args.batch_size} ä¸ª/æ‰¹, {workers} ä¸ªå¹¶è¡Œè¿›ç¨‹",
          file=sys.stderr)

    # â”€â”€ Parallel analysis â”€â”€
    start_time = datetime.now()
    results = {}

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(analyze_batch, batch, i + 1, total): i
            for i, batch in enumerate(batches)
        }

        for future in as_completed(futures):
            idx = futures[future]
            result = future.result()
            results[idx] = result

            emoji = "âœ“" if result["status"] == "success" else "âš ï¸"
            batch_sessions = batches[idx]
            t_start = batch_sessions[0].get("time_start", "?")
            t_end = batch_sessions[-1].get("time_start", "?")
            elapsed = (datetime.now() - start_time).seconds
            print(f"  {emoji} æ‰¹æ¬¡ {idx + 1}/{total} å®Œæˆ ({t_start} ~ {t_end}) [{elapsed}s]",
                  file=sys.stderr)

    elapsed_total = (datetime.now() - start_time).seconds
    print(f"âœ“ æ‰€æœ‰æ‰¹æ¬¡åˆ†æå®Œæˆ (æ€»è€—æ—¶ {elapsed_total}s)", file=sys.stderr)

    # â”€â”€ Assemble chapter 3 â”€â”€
    parts = ["## ä¸‰ã€å„ä¼šè¯è¯¦æƒ…\n"]
    for i in sorted(results.keys()):
        parts.append(results[i]["content"])

    chapter3 = "\n\n".join(parts)

    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(chapter3)

    # â”€â”€ Print summary to stdout â”€â”€
    success = sum(1 for r in results.values() if r["status"] == "success")
    summary = {
        "status": "complete",
        "total_sessions": len(sessions),
        "total_batches": total,
        "success": success,
        "failed": total - success,
        "elapsed_seconds": elapsed_total,
        "output_file": args.output,
    }
    print(json.dumps(summary, ensure_ascii=False))


if __name__ == "__main__":
    main()
